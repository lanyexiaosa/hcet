{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys, pdb, os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import sonnet as snt\n",
    "from sklearn.metrics import roc_auc_score,average_precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import binarize\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "print(\"Tensorflow verion:\",tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(input_path,min_threshold, max_threshold, seed, seqs_name='.seqs', labels_name='.labels'):\n",
    "    seqs = pickle.load(open(input_path + seqs_name, 'rb'))\n",
    "    labels = pickle.load(open(input_path + labels_name, 'rb'))\n",
    "\n",
    "    new_seqs = []\n",
    "    new_labels = []\n",
    "    new_demos= []\n",
    "    for seq, label in zip(seqs, labels):\n",
    "        if len(seq) < min_threshold or len(seq) > max_threshold:\n",
    "            continue\n",
    "        else:\n",
    "            new_seqs.append(seq)\n",
    "            new_labels.append(label)\n",
    "    \n",
    "    \n",
    "    seqs = new_seqs\n",
    "    labels = new_labels\n",
    "\n",
    "    temp_seqs, test_seqs, temp_labels, test_labels = train_test_split(seqs, labels, test_size=0.2, random_state=seed)\n",
    "    train_seqs, valid_seqs, train_labels, valid_labels = train_test_split(temp_seqs, temp_labels, test_size=0.1, random_state=seed)\n",
    "\n",
    "    train_size = int(len(train_seqs))\n",
    "    train_seqs = train_seqs[:train_size]\n",
    "    train_labels = train_labels[:train_size]\n",
    "    \n",
    "    #sort patient's sequence by its visit length\n",
    "    def len_argsort(seq):\n",
    "        return sorted(range(len(seq)), key=lambda x: len(seq[x]))\n",
    "\n",
    "    sorted_index = len_argsort(train_seqs)\n",
    "    train_seqs = [train_seqs[i] for i in sorted_index]\n",
    "    train_labels = [train_labels[i] for i in sorted_index]\n",
    "\n",
    "    sorted_index = len_argsort(valid_seqs)\n",
    "    valid_seqs = [valid_seqs[i] for i in sorted_index]\n",
    "    valid_labels = [valid_labels[i] for i in sorted_index]\n",
    "\n",
    "    sorted_index = len_argsort(test_seqs)\n",
    "    test_seqs = [test_seqs[i] for i in sorted_index]\n",
    "    test_labels = [test_labels[i] for i in sorted_index]\n",
    "\n",
    "\n",
    "    return train_seqs, train_labels, valid_seqs, valid_labels, test_seqs, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model\n",
    "def build_model(options):\n",
    "    #different choice of activation funciton\n",
    "    if options['emb_activation'] == 'sigmoid':\n",
    "        emb_activation = tf.nn.sigmoid\n",
    "    else:\n",
    "        emb_activation = tf.nn.relu\n",
    "\n",
    "    if options['visit_activation'] == 'sigmoid':\n",
    "        visit_activation = tf.nn.sigmoid\n",
    "    else:\n",
    "        visit_activation = tf.nn.relu\n",
    "        \n",
    "    #embedding matrix, dx for icd code; rx for medication code; pr for procedure code\n",
    "    #num is the unique number of each code; size is the embedding size to be chosen, 200 as default\n",
    "    W_emb_icd = tf.get_variable('W_emb_icd', shape=(options['num_icd'], options['icd_emb_size']), dtype=tf.float32)\n",
    "    W_emb_cpt = tf.get_variable('W_emb_cpt', shape=(options['num_cpt'], options['cpt_emb_size']), dtype=tf.float32)\n",
    "    W_emb_med = tf.get_variable('W_emb_med', shape=(options['num_med'], options['med_emb_size']), dtype=tf.float32)\n",
    "    W_emb_demo = tf.get_variable('W_emb_demo', shape=(options['num_demo'], options['demo_emb_size']), dtype=tf.float32)\n",
    "    W_emb_topic = tf.get_variable('W_emb_topic', shape=(options['num_topic'], options['topic_emb_size']), dtype=tf.float32)\n",
    "\n",
    "    #input vector for dx,rx,pr \n",
    "    icd_var = tf.placeholder(tf.int32, shape=(None, options['batch_size'], options['max_icd_per_visit']), name='icd_var')\n",
    "    cpt_var = tf.placeholder(tf.int32, shape=(None, options['batch_size'], options['max_cpt_per_visit']), name='cpt_var')\n",
    "    med_var = tf.placeholder(tf.int32, shape=(None, options['batch_size'], options['max_med_per_visit']), name='med_var')\n",
    "    demo_var = tf.placeholder(tf.int32, shape=(None, options['batch_size'], options['demo_per_visit']), name='demo_var')\n",
    "    topic_var = tf.placeholder(tf.int32, shape=(None, options['batch_size'], options['max_topic_per_visit']), name='demo_var')\n",
    "\n",
    "    #mask for residual connection\n",
    "    icd_mask = tf.placeholder(tf.float32, shape=(None, options['batch_size'], options['max_icd_per_visit']), name='icd_mask')\n",
    "    cpt_mask = tf.placeholder(tf.float32, shape=(None, options['batch_size'], options['max_cpt_per_visit']), name='cpt_mask')\n",
    "    med_mask = tf.placeholder(tf.float32, shape=(None, options['batch_size'], options['max_med_per_visit']), name='med_mask')\n",
    "    demo_mask = tf.placeholder(tf.float32, shape=(None, options['batch_size'], options['demo_per_visit']), name='demo_mask')\n",
    "    topic_mask = tf.placeholder(tf.float32, shape=(None, options['batch_size'], options['max_topic_per_visit']), name='demo_mask')\n",
    "\n",
    "    # lookup is the multiplication to retrieve embeddings from embedding matrix\n",
    "    #second param is the index to retrieve\n",
    "    icd_visit = tf.nn.embedding_lookup(W_emb_icd, tf.reshape(icd_var, (-1, options['max_icd_per_visit'])))\n",
    "    icd_visit = emb_activation(icd_visit)\n",
    "    icd_visit = icd_visit * tf.reshape(icd_mask, (-1, options['max_icd_per_visit']))[:, :, None] ####Masking####\n",
    "    icd_visit = tf.reduce_sum(icd_visit, axis=1)\n",
    "    \n",
    "   # dx_visit = tf.reshape(dx_visit, (-1, options['dx_emb_size'])) # dx_emb_size=200\n",
    "\n",
    "    cpt_visit = tf.nn.embedding_lookup(W_emb_cpt, tf.reshape(cpt_var, (-1, options['max_cpt_per_visit'])))\n",
    "    cpt_visit = emb_activation(cpt_visit)\n",
    "    cpt_visit = cpt_visit * tf.reshape(cpt_mask, (-1, options['max_cpt_per_visit']))[:, :, None] ####Masking####\n",
    "    cpt_visit = tf.reduce_sum(cpt_visit, axis=1)\n",
    "    \n",
    "    med_visit = tf.nn.embedding_lookup(W_emb_med, tf.reshape(med_var, (-1, options['max_med_per_visit'])))\n",
    "    med_visit = emb_activation(med_visit)\n",
    "    med_visit = med_visit * tf.reshape(med_mask, (-1, options['max_med_per_visit']))[:, :, None] ####Masking####\n",
    "    med_visit = tf.reduce_sum(med_visit, axis=1)\n",
    "    \n",
    "    demo_visit = tf.nn.embedding_lookup(W_emb_demo, tf.reshape(demo_var, (-1, options['demo_per_visit'])))\n",
    "    demo_visit = emb_activation(demo_visit)\n",
    "    demo_visit = demo_visit * tf.reshape(demo_mask, (-1, options['demo_per_visit']))[:, :, None] ####Masking####\n",
    "    demo_visit = tf.reduce_sum(demo_visit, axis=1)\n",
    "    \n",
    "    topic_visit = tf.nn.embedding_lookup(W_emb_topic, tf.reshape(topic_var, (-1, options['max_topic_per_visit'])))\n",
    "    topic_visit = emb_activation(topic_visit)\n",
    "    topic_visit = topic_visit * tf.reshape(topic_mask, (-1, options['max_topic_per_visit']))[:, :, None] ####Masking####\n",
    "    topic_visit = tf.reduce_sum(topic_visit, axis=1)\n",
    "    \n",
    "    # adding attention weights for each EHR type\n",
    "    \n",
    "    attention = tf.get_variable('attens',shape=(1,5),dtype=tf.float32, \n",
    "                                initializer=tf.constant_initializer([[1.0,1.0,1.0,1.0,1.0]]))\n",
    "    attention_weights = tf.nn.softmax(attention)\n",
    "    \n",
    "    EHR_obj = tf.gather_nd(attention_weights,[[0,0]])*icd_visit + tf.gather_nd(attention_weights,[[0,1]])*cpt_visit \\\n",
    "    + tf.gather_nd(attention_weights,[[0,2]])*med_visit + tf.gather_nd(attention_weights,[[0,3]])*demo_visit + \\\n",
    "    tf.gather_nd(attention_weights,[[0,4]])*topic_visit\n",
    "    W_dx = snt.Sequential([snt.Linear(output_size=options['ehr_emb_size'], name='W_ehr'), visit_activation])\n",
    "    EHR_obj = W_dx(EHR_obj)\n",
    "    \n",
    "#     use the following codes if not use attention weights   \n",
    "#     # sum of ICD-9,medication and cpt codes\n",
    "#     EHR_obj = icd_visit + cpt_visit + med_visit + demo_visit + topic_visit\n",
    "#     W_dx = snt.Sequential([snt.Linear(output_size=options['ehr_emb_size'], name='W_ehr'), visit_activation])\n",
    "#     EHR_obj = W_dx(EHR_obj)\n",
    "    \n",
    "    seq_visit = tf.reshape(EHR_obj, (-1, options['batch_size'], options['visit_emb_size']))     \n",
    "    seq_length = tf.placeholder(tf.int32, shape=(options['batch_size']), name='seq_length')\n",
    "    rnn = snt.GRU(options['rnn_size'], name='emb2rnn')\n",
    "    rnn2pred = snt.Sequential([snt.Linear(output_size=options['output_size'], name='rnn2pred'), tf.nn.sigmoid])\n",
    "    \n",
    "    \n",
    "    #output\n",
    "    _, final_states = tf.nn.dynamic_rnn(rnn, seq_visit, dtype=tf.float32, time_major=True, sequence_length=seq_length)\n",
    "    preds = tf.squeeze(rnn2pred(final_states))\n",
    "    labels = tf.placeholder(tf.float32, shape=(options['batch_size']), name='labels')\n",
    "    loss = -tf.reduce_mean(labels * tf.log(preds + 1e-10) + (1. - labels) * tf.log(1. - preds + 1e-10))\n",
    "    \n",
    "    \n",
    "    #get tensors for this model\n",
    "    input_tensors = (icd_var, cpt_var, med_var,demo_var,topic_var)\n",
    "    label_tensors = labels\n",
    "    mask_tensors = (icd_mask, cpt_mask, med_mask,demo_mask,topic_mask)\n",
    "    loss_tensors = loss\n",
    "\n",
    "    return input_tensors, label_tensors, mask_tensors, loss_tensors, seq_length, preds, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to run test set\n",
    "def run_test(seqs, label_seqs, sess, preds_T, input_PHs, label_PHs, mask_PHs, seq_length_PH, loss_T, options):\n",
    "    all_losses = []\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    batch_size = options['batch_size']\n",
    "    \n",
    "    for idx in range(len(label_seqs) // batch_size):\n",
    "        batch_x = seqs[idx*batch_size:(idx+1)*batch_size]\n",
    "        batch_y = label_seqs[idx*batch_size:(idx+1)*batch_size]\n",
    "        inputs, masks, seq_length = preprocess_batch(batch_x, options)\n",
    "        \n",
    "        preds, loss = sess.run([preds_T, loss_T],\n",
    "                feed_dict={\n",
    "                    input_PHs[0]:inputs[0],\n",
    "                    input_PHs[1]:inputs[1],\n",
    "                    input_PHs[2]:inputs[2],\n",
    "                    input_PHs[3]:inputs[3],\n",
    "                    input_PHs[4]:inputs[4],\n",
    "                    mask_PHs[0]:masks[0],\n",
    "                    mask_PHs[1]:masks[1],\n",
    "                    mask_PHs[2]:masks[2],\n",
    "                    mask_PHs[3]:masks[3],\n",
    "                    mask_PHs[4]:masks[4],\n",
    "                    label_PHs:batch_y,      # only feed the label for global prediction in our case\n",
    "                    seq_length_PH:seq_length,\n",
    "                    }\n",
    "                )\n",
    "        \n",
    "        all_losses.append(loss)\n",
    "        all_preds.extend(list(preds))\n",
    "        all_labels.extend(batch_y)\n",
    "        \n",
    "    auc = roc_auc_score(all_labels, all_preds)\n",
    "    aucpr = average_precision_score(all_labels, all_preds)\n",
    "    accuracy = (np.array(all_labels) == np.squeeze(binarize(np.array(all_preds).reshape(-1, 1), threshold=.5))).mean()\n",
    "    return np.mean(all_losses), auc, aucpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_path,\n",
    "          output_path,\n",
    "          batch_size,\n",
    "          num_iter,\n",
    "          eval_period,\n",
    "          rnn_size,\n",
    "          output_size,\n",
    "          learning_rate,\n",
    "          random_seed,\n",
    "          split_seed,\n",
    "          emb_activation,\n",
    "          visit_activation,\n",
    "          num_icd,\n",
    "          num_cpt,\n",
    "          num_med,\n",
    "          num_demo,\n",
    "          num_topic,\n",
    "          icd_emb_size,\n",
    "          cpt_emb_size,\n",
    "          med_emb_size,\n",
    "          demo_emb_size,\n",
    "          topic_emb_size,\n",
    "          ehr_emb_size,\n",
    "          visit_emb_size,\n",
    "          max_icd_per_visit,\n",
    "          max_cpt_per_visit,\n",
    "          max_med_per_visit,\n",
    "          max_topic_per_visit,\n",
    "          demo_per_visit,\n",
    "          regularize,\n",
    "          min_threshold,\n",
    "          max_threshold):\n",
    "            \n",
    "    #copy local parameters\n",
    "    options = locals().copy()\n",
    "    #build the model\n",
    "    input_PHs, label_PHs, mask_PHs, loss_T, seq_length_PH, preds_T, attention_weights = build_model(options)\n",
    "    \n",
    "    #add L2 regularization\n",
    "    all_vars = tf.trainable_variables()\n",
    "    L2_loss = tf.constant(0.0, dtype=tf.float32)\n",
    "    for var in all_vars:\n",
    "        if len(var.shape) < 2:\n",
    "            continue\n",
    "        L2_loss += tf.reduce_sum(var ** 2)\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=options['learning_rate'])\n",
    "    minimize_op = optimizer.minimize(loss_T + regularize * L2_loss)\n",
    "    \n",
    "    train_seqs, train_labels, valid_seqs, valid_labels, test_seqs, test_labels = load_data(\n",
    "            options['input_path'],\n",
    "            min_threshold=options['min_threshold'],\n",
    "            max_threshold=options['max_threshold'],\n",
    "            seed=options['split_seed'])\n",
    "    \n",
    "    #save the file and check point\n",
    "    saver = tf.train.Saver(max_to_keep=1)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        best_valid_loss = 100000.0\n",
    "        best_test_loss = 100000.0\n",
    "        best_valid_auc = 0.0\n",
    "        best_test_auc = 0.0\n",
    "        best_valid_aucpr = 0.0\n",
    "        best_test_aucpr = 0.0\n",
    "        \n",
    "        for train_iter in range(options['num_iter']+1):\n",
    "            batch_x, batch_y = sample_batch(train_seqs, train_labels, options['batch_size'])\n",
    "            inputs, masks, seq_length = preprocess_batch(batch_x, options)\n",
    "            \n",
    "            _, preds, losses = sess.run([minimize_op, preds_T, loss_T],\n",
    "                    feed_dict={\n",
    "                        input_PHs[0]:inputs[0],\n",
    "                        input_PHs[1]:inputs[1],\n",
    "                        input_PHs[2]:inputs[2],\n",
    "                        input_PHs[3]:inputs[3],\n",
    "                        input_PHs[4]:inputs[4],\n",
    "                        mask_PHs[0]:masks[0],\n",
    "                        mask_PHs[1]:masks[1],\n",
    "                        mask_PHs[2]:masks[2],\n",
    "                        mask_PHs[3]:masks[3],\n",
    "                        mask_PHs[4]:masks[4],\n",
    "                        label_PHs:batch_y,\n",
    "                        seq_length_PH:seq_length,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "            if train_iter > 0 and train_iter % options['eval_period'] == 0:\n",
    "                #validation process\n",
    "                valid_loss, valid_auc, valid_aucpr = run_test(valid_seqs, valid_labels, sess, preds_T, \n",
    "                                                              input_PHs, label_PHs, mask_PHs, seq_length_PH, \n",
    "                                                              loss_T, options)\n",
    "                #test the model\n",
    "                if valid_loss < best_valid_loss:\n",
    "                    test_loss, test_auc, test_aucpr = run_test(test_seqs, test_labels, sess, preds_T, input_PHs,\n",
    "                                                               label_PHs, mask_PHs, seq_length_PH, loss_T, options)\n",
    "                    best_valid_loss = valid_loss\n",
    "                    best_valid_auc = valid_auc\n",
    "                    best_valid_aucpr = valid_aucpr\n",
    "                    best_test_loss = test_loss\n",
    "                    best_test_auc = test_auc\n",
    "                    best_test_aucpr = test_aucpr\n",
    "                    best_attention=attention_weights.eval()\n",
    "                    print(\"attention weights:\", best_attention)\n",
    "                    print('\\n')\n",
    "                    \n",
    "                    savePath = saver.save(sess, output_path + '/r' + str(random_seed) + 's' + str(split_seed) \n",
    "                                          + '/model', global_step=train_iter)\n",
    "                print('Steps: %d, valid_loss:%f, valid_auc:%f, valid_aucpr:%f' % (train_iter, valid_loss, valid_auc,\n",
    "                                                                                 valid_aucpr))\n",
    "                \n",
    "        return best_valid_loss, best_test_loss, best_valid_auc, best_test_auc, best_valid_aucpr, best_test_aucpr, best_attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_batch(patients, options):\n",
    "         \n",
    "    lengths = np.array([len(seq) for seq in patients])# visit length for each patient\n",
    "    max_length = np.max(lengths)# max length of visit\n",
    "    num_samples = len(patients)\n",
    "    \n",
    "    icd = np.zeros((num_samples, max_length, options['max_icd_per_visit'])).astype('int32')\n",
    "    cpt = np.zeros((num_samples, max_length, options['max_cpt_per_visit'])).astype('int32')\n",
    "    med = np.zeros((num_samples, max_length, options['max_med_per_visit'])).astype('int32')\n",
    "    demo = np.zeros((num_samples, max_length, options['demo_per_visit'])).astype('int32')\n",
    "    topic = np.zeros((num_samples, max_length, options['max_topic_per_visit'])).astype('int32')\n",
    "\n",
    "    icd_mask = np.zeros((num_samples, max_length, options['max_icd_per_visit'])).astype('float32')\n",
    "    cpt_mask = np.zeros((num_samples, max_length, options['max_cpt_per_visit'])).astype('float32')\n",
    "    med_mask = np.zeros((num_samples, max_length, options['max_med_per_visit'])).astype('float32')\n",
    "    demo_mask = np.zeros((num_samples, max_length, options['demo_per_visit'])).astype('float32')\n",
    "    topic_mask = np.zeros((num_samples, max_length, options['max_topic_per_visit'])).astype('float32')\n",
    "            \n",
    "            \n",
    "    for i, patient in enumerate(patients):\n",
    "        for j, visit in enumerate(patient):\n",
    "            icd_ind=0\n",
    "            cpt_ind=0\n",
    "            med_ind=0\n",
    "            demo_ind=0\n",
    "            topic_ind=0\n",
    "            for code in visit:\n",
    "                if code<85: # code for demo\n",
    "                    demo[i,j,demo_ind] = code\n",
    "                    demo_mask[i,j,demo_ind] = 1.\n",
    "                    demo_ind+=1\n",
    "                \n",
    "                elif code<284 and code>=85: # code for icd-9 \n",
    "                    icd[i,j,icd_ind] = code-85\n",
    "                    icd_mask[i,j,icd_ind] = 1.\n",
    "                    icd_ind+=1\n",
    "                \n",
    "                elif code>=284 and code<2282:   #code for cpt\n",
    "                    cpt[i,j,cpt_ind] = code-284\n",
    "                    cpt_mask[i,j,cpt_ind] = 1.\n",
    "                    cpt_ind+=1\n",
    "            \n",
    "                elif code>=2282 and code<2773:   #code for medication\n",
    "                    med[i,j,med_ind] = code-2282\n",
    "                    med_mask[i,j,med_ind] = 1.\n",
    "                    med_ind+=1\n",
    "                \n",
    "                #code for topic feature\n",
    "                else:\n",
    "                    topic[i,j,topic_ind] = code-2773\n",
    "                    topic_mask[i,j,topic_ind] = 1.\n",
    "                    topic_ind+=1    \n",
    "                    \n",
    "    icd = np.transpose(icd, (1, 0, 2)) #time-major RNN\n",
    "    cpt = np.transpose(cpt, (1, 0, 2))\n",
    "    med = np.transpose(med, (1, 0, 2))\n",
    "    demo = np.transpose(demo, (1, 0, 2))\n",
    "    topic = np.transpose(topic, (1, 0, 2))\n",
    "    \n",
    "    icd_mask = np.transpose(icd_mask, (1, 0, 2))\n",
    "    cpt_mask = np.transpose(cpt_mask, (1, 0, 2))\n",
    "    med_mask = np.transpose(med_mask, (1, 0, 2))\n",
    "    demo_mask = np.transpose(demo_mask, (1, 0, 2))\n",
    "    topic_mask = np.transpose(topic_mask, (1, 0, 2))\n",
    "    \n",
    "    lengths = np.array(lengths).astype('int32')\n",
    "\n",
    "    inputs = (icd, cpt, med, demo, topic)\n",
    "    masks = (icd_mask, cpt_mask, med_mask, demo_mask, topic_mask)\n",
    "    \n",
    "    return inputs, masks, lengths\n",
    "    \n",
    "def sample_batch(seqs, labels, batch_size):\n",
    "    idx = np.random.randint(0, len(seqs) - batch_size + 1)\n",
    "    return seqs[idx:idx+batch_size], labels[idx:idx+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main function to run the model\n",
    "\n",
    "#set parameters\n",
    "input_path='data_oneyear_all'\n",
    "output_path = 'outfile_oneyear_all_attent'\n",
    "log_path = 'log_file'\n",
    "\n",
    "valid_losses = []\n",
    "test_losses = []\n",
    "valid_aucs = []\n",
    "test_aucs = []\n",
    "valid_aucprs = []\n",
    "test_aucprs = []\n",
    "attentions=[]\n",
    "\n",
    "\n",
    "for i in range(1):\n",
    "    tf.set_random_seed(i)\n",
    "    np.random.seed(i)\n",
    "    for j in range(10):\n",
    "        os.makedirs(output_path + '/r' + str(i) + 's' + str(j) + '/')\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        valid_loss, test_loss, valid_auc, test_auc, valid_aucpr, test_aucpr, attention_weights = train(\n",
    "            input_path=input_path,\n",
    "            output_path=output_path,\n",
    "            batch_size=50,\n",
    "            num_iter=2000,\n",
    "            eval_period=100,\n",
    "            rnn_size=256,\n",
    "            output_size=1,\n",
    "            learning_rate=1e-4,\n",
    "            random_seed=i,\n",
    "            split_seed=j,\n",
    "            emb_activation='relu',\n",
    "            visit_activation='relu',\n",
    "            num_icd=199,\n",
    "            num_cpt=1998,\n",
    "            num_med=491,\n",
    "            num_demo=85,\n",
    "            num_topic=100,\n",
    "            icd_emb_size=200,\n",
    "            cpt_emb_size=200,\n",
    "            med_emb_size=200,\n",
    "            demo_emb_size=200,\n",
    "            topic_emb_size=200,\n",
    "            ehr_emb_size=256,\n",
    "            visit_emb_size=256,\n",
    "            max_icd_per_visit=69,\n",
    "            max_cpt_per_visit=106,\n",
    "            max_med_per_visit=14,\n",
    "            max_topic_per_visit=30,\n",
    "            demo_per_visit=2,\n",
    "            regularize=1e-4,\n",
    "            min_threshold=2,\n",
    "            max_threshold=180)\n",
    "        \n",
    "        valid_losses.append(valid_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        valid_aucs.append(valid_auc)\n",
    "        test_aucs.append(test_auc)\n",
    "        valid_aucprs.append(valid_aucpr)\n",
    "        test_aucprs.append(test_aucpr)\n",
    "        attentions.append(attention_weights)\n",
    "        buf  = \"valid_loss:%f, test_loss:%f, valid_auc:%f, test_auc:%f, valid_aucpr:%f, test_aucpr:%f\" % (valid_loss, test_loss, valid_auc, test_auc, valid_aucpr, test_aucpr)\n",
    "        with open(log_path + '.log', 'a') as outfd: outfd.write(buf + '\\n')\n",
    "        print(buf)\n",
    "        \n",
    "buf  = \"mean_valid_loss:%f, mean_test_loss:%f, mean_valid_auc:%f, mean_test_auc:%f, mean_valid_aucpr:%f, mean_test_aucpr:%f\" % (np.mean(valid_losses), np.mean(test_losses), np.mean(valid_aucs), np.mean(test_aucs), np.mean(valid_aucprs), np.mean(test_aucprs))\n",
    "with open(log_path + '.log', 'a') as outfd: outfd.write(buf + '\\n')\n",
    "print(buf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test ROCAUC:\",test_aucs)\n",
    "print(\"Test PRAUC:\",test_aucprs)\n",
    "print(\"attention weights:\",attentions)\n",
    "print(\"mean Test ROCAUC:\",np.mean(test_aucs))\n",
    "print(\"mean Test PRCAUC:\",np.mean(test_aucprs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=[0,0,0,0,0]\n",
    "for array in attentions:\n",
    "    res+=array\n",
    "\n",
    "print(res/10)   \n",
    "save_attent=np.squeeze(np.array(attentions))\n",
    "np.savetxt(output_path+'.csv',save_attent,delimiter=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
